{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_file_path = '/content/drive/MyDrive/archive.zip'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeq6KlGZbrvT",
        "outputId": "40f0dc96-f03b-4817-912a-1339ad88ad7b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/archive.zip'"
      ],
      "metadata": {
        "id": "zwTazbuRbshW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import random"
      ],
      "metadata": {
        "id": "LJ6zZkTlOZ7p"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PaperclipDataset(Dataset):\n",
        "    def __init__(self, train_image_ids, train_csv, images_folder):\n",
        "        self.train_image_ids = train_image_ids\n",
        "        self.train_df = pd.read_csv(train_csv)\n",
        "        self.images_folder = images_folder\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # reading image from path and label/count for the image ids\n",
        "        image_id = self.train_image_ids[idx]\n",
        "        image_path = f\"{self.images_folder}/clips-{image_id}.png\"\n",
        "        image = Image.open(image_path)\n",
        "        label = self.train_df[self.train_df['id'] == image_id]['clip_count'].values[0]\n",
        "\n",
        "\n",
        "        image_tensor = transforms.ToTensor()(image)\n",
        "        print(\"Original image shape:\", image_tensor.shape)\n",
        "        # print(image_tensor.size())\n",
        "\n",
        "        image_tensor = transforms.Resize((28, 28))(image_tensor)\n",
        "        # print(image_tensor.size())\n",
        "        image_tensor = image_tensor[:3, :, :]\n",
        "        # print(image_tensor.size())\n",
        "\n",
        "        x = 32  # Last two digits of my roll number (20EE38032)\n",
        "\n",
        "        # random rotation\n",
        "        #image_tensor = transforms.RandomRotation(degrees=(-x, x));\n",
        "\n",
        "        # random horizontal flipping\n",
        "        #image_tensor = transforms.RandomHorizontalFlip(p=x/ 100);\n",
        "        #print(\"Original image shape:\", image_tensor.shape)\n",
        "        # Iterate over the training dataset to compute mean and standard deviation\n",
        "        #print(len(image_tensor))\n",
        "        #image_tensor  = torch.stack(list(image_tensor), dim=0)\n",
        "\n",
        "        # Normalize each channel of the image\n",
        "        #for i in range(3):\n",
        "           # image_tensor[i] = (image_tensor[i] - self.mean[i]) / self.std[i]\n",
        "\n",
        "        #Convert the transformed image to a flattened 1D tensor\n",
        "        image_tensor = torch.flatten(image_tensor)\n",
        "        return image_id, image_tensor, label"
      ],
      "metadata": {
        "id": "hCo9mCAvOVJk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataloader"
      ],
      "metadata": {
        "id": "VHPfI6FIty-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_csv = '/content/drive/MyDrive/extracted_files/train.csv'  # Path to train.csv\n",
        "images_folder = '/content/drive/MyDrive/extracted_files/clips-data-2020/clips'  # Path to the images folder"
      ],
      "metadata": {
        "id": "IWy4RYk3S8ja"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP model and summary drawn"
      ],
      "metadata": {
        "id": "YZLr4blFcZxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        nn.Module.__init__(self)\n",
        "        self.layers = []\n",
        "        self.n=int(math.log(input_size)/math.log(8))\n",
        "        hidden_size = math.floor(input_size/8)\n",
        "        for i in range(self.n):\n",
        "            self.layers.append(nn.Linear(input_size, hidden_size))\n",
        "            self.layers.append(nn.ReLU())\n",
        "            input_size = hidden_size\n",
        "            hidden_size = math.floor(input_size/ 8)\n",
        "\n",
        "        self.layers.append(nn.Linear(input_size,1))\n",
        "        self.model = nn.Sequential(*self.layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.layers)):\n",
        "          x=self.layers[i](x)\n",
        "          #print(x.size())\n",
        "          # x= nn.functional.relu(x)\n",
        "        # x = int(self.layers[i+1])\n",
        "        return x\n",
        "        #return self.model(x)\n",
        "\n",
        "input_size = 28*28*3\n",
        "model = MLP(input_size)\n",
        "\n",
        "# Printing the model architecture\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "ADGU-dVAVJKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735f4088-e301-4445-8f4b-7f01e4d2d38f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=2352, out_features=294, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=294, out_features=36, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=36, out_features=4, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=4, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "splitting into training set, validation set and testing set"
      ],
      "metadata": {
        "id": "YZs7llPVwfAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "# Load image ids and corresponding counts from train.csv\n",
        "df = pd.read_csv(train_csv)\n",
        "img_ids = df['id'].tolist()\n",
        "\n",
        "train_ids, val_ids = train_test_split(img_ids, test_size=0.2, random_state=42)\n",
        "#print(np.array(val_ids).shape)\n",
        "train_dataset= PaperclipDataset(train_ids, train_csv, images_folder)\n",
        "val_dataset= PaperclipDataset(val_ids, train_csv, images_folder)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "dal_loader= DataLoader(val_dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "FnKrlslVJQAh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MLP(input_size).to(device)"
      ],
      "metadata": {
        "id": "-Rbo9uGMclb-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _,images, labels in train_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    #print(images.shape)\n",
        "    #print(labels.shape)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "    #outputs=tensor.cpu()\n",
        "    outputs= [t.cpu().detach().numpy() for t in outputs]\n",
        "    outputs = np.vstack(outputs)\n",
        "    #print(outputs)\n",
        "\n",
        "    for i in range(5):\n",
        "         print(\"Predicted Count:\", outputs[i].item())\n",
        "         print(\"Ground Truth Count:\", labels[i].item())\n",
        "    break\n",
        "\n",
        "#summary of the model is printed before"
      ],
      "metadata": {
        "id": "6x8EFgbBdI-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efc40358-7841-4c93-8732-32c433963e65"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Count: 0.47074243426322937\n",
            "Ground Truth Count: 69\n",
            "Predicted Count: 0.4691500961780548\n",
            "Ground Truth Count: 25\n",
            "Predicted Count: 0.46871283650398254\n",
            "Ground Truth Count: 62\n",
            "Predicted Count: 0.4699503183364868\n",
            "Ground Truth Count: 35\n",
            "Predicted Count: 0.4686519205570221\n",
            "Ground Truth Count: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sampling Randomly from their respective sets"
      ],
      "metadata": {
        "id": "SWR5lw5LwVph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_csv='/content/drive/MyDrive/extracted_files/test.csv'\n",
        "df = pd.read_csv(test_csv)\n",
        "test_ids = df['id'].tolist()\n",
        "#print(test_ids)\n",
        "\n",
        "random.shuffle(train_ids)\n",
        "random.shuffle(val_ids)\n",
        "random.shuffle(test_ids)\n",
        "\n",
        "train_sample=train_ids[:2000]\n",
        "val_sample=val_ids[:250]\n",
        "test_sample=test_ids[:250]"
      ],
      "metadata": {
        "id": "Zu9MD0aUMbEZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train loader and validation loader"
      ],
      "metadata": {
        "id": "65ZPMFR7wQ3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset= PaperclipDataset(train_sample, train_csv, images_folder)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "val_dataset= PaperclipDataset(val_sample, train_csv, images_folder)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "QD6NIY2I37NL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "f6YBfu8NW8Qe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "6zoM5pds1ke7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "Tizlmvcm2mii"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = np.zeros(num_epochs)\n",
        "val_losses = np.zeros(num_epochs)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for _, images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        labels = labels.float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for _, images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            val_loss += criterion(outputs.squeeze(), labels).item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    train_losses[epoch] = train_loss\n",
        "    val_losses[epoch] = val_loss\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "bdkCHjDJ7qet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(0,20),train_losses)\n",
        "plt.xlabel('No of epoch')\n",
        "plt.ylabel('Train Loss')\n",
        "\n",
        "plt.plot(range(0,20),val_losses)\n",
        "plt.xlabel('No of epoch')\n",
        "plt.ylabel('validation Loss')"
      ],
      "metadata": {
        "id": "sON-CTXcNuG6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}